# NanoporeAnalysis
This module is meant for processing Nanopore sequencing reads into usable data. This currently requires that the sequencing library was prepared with poly-dT barcodes with an integrated PCR primer sequence and a matching sequence ligated to the 5' end of the cDNA, and this is intended for use with transcriptomic analysis. Limited statistical analysis tools are also included for initial investigation and inspection of the data.
The workflow can most easily begin with .sam files output by the Dorado basecaller provided by Nanopore at https://github.com/nanoporetech/dorado/. Some functions are provided for running Dorado basecaller on a distributed HPC system for faster basecalling, but these functions are currently designed around the Ohio SuperComputer system, which is a linux-based system using SLURM. Using these functions is not necessary and will not change the resulting data compared to running Dorado in any other manner and just starting with the resulting sam files.
You will need a python environment as listed in the included environment.yml file, and you will need to download Dorado, at least version 0.9.1. You will also need reference sequences if you want to map to genes.
To begin with .pod5 files and run Dorado in parallel: 
    Run basecall.move_pod5s() to copy your pod5 files into groups for multiple jobs of Dorado to use. Direct this function to a folder containing pod5 files and to an output folder to hold the folders of grouped pod5 files. Set the workers variable here to the number of Dorado jobs you'd like to run. 
    Run basecall.dorado_slurm() to run parallel Dorado jobs on a SLURM HPC system. This is designed around the Ohio SuperComputer system, but you can edit the script settings within basecall.py to work with other SLURM systems. These Dorado jobs will take the grouped pod5 files from move_pod5s() and generate .sam files containing the basecalled reads, which can then be used for next steps.
To begin with .sam files (or continue with the .sam files from above):
    Run local_io.build_parquet_dataset_from_sam() to convert the sam files into a parquet file dataset. Parquet is a highly efficient, compressed, tabular file and dataset format used by Apache's arrow system, which is implemented in their pyarrow python module. This conversion is necessary for further steps.
    Run debarcode.debarcode(). This function asigns reads to the barcode sequences that were used in the sequencing library prep, which is necessary for multiplexed libraries in order to parse out which reads came from what samples. This function must be pointed to the parquet dataset created above, and you must provide the UMIs (unique molecular identifiers) and SSP (strand switching primer) used in your barcodes. The UMIs are the unique sequences within the barcodes that can be used to identify the reads, and the SSP sequence is the integrated PCR primer sequence on the end of the primers that should have also been ligated to the 5' end of the cDNAs generated from reverse transcription with the barcodes such that there is an SSP sequence at both ends of the cDNA. The UMIs and SSP should be given as the reverse complement of the actual DNA primer sequences such that the primer is read polyA-UMI-SSP. This is important for proper alignment. UMIs are given as a list of (name, sequence) pairs. A python dict mapping barcode/UMI names to sample IDs can be provided with sample_dict. The various parameters for finding and accepting barcode assignments are given reasonable default values, but you may need to change them for your purposes. This function can be parallelized in a multi-core environment. This function will directly edit the parquet dataset that you point it to, adding the new barcode and sample assignments to each read.
    Run map_count.minimap2(). This function runs the sequence alignment tool minimap2 (https://github.com/lh3/minimap2/tree/master) to map each read against a sequence reference. This function will also assign gene IDs based on the provided references, which are intended to be the .fna files provided by the Genome Research Consortium. If using a transcriptomic reference, this function will pull gene IDs from the transcript sequence names in the reference file. If using a genomic reference, you'll also need to provide a .bed file (which can also be grabbed from the GRC) in order to assign gene IDs. The preset given to minimap2 is set to splice by default, which is best for transcriptomic data like this. This function can be run in parallel, but minimap2 is also very memory-intensive, currently requiring about 40GB of RAM per task.
    Run map_count.count_mapped_reads(). This function reads through the parquet dataset and tallies up the numer of times each gene is seen for each barcode. A sample_dict mapping barcodes to sample names is currently required for this function. The max_match parameter can also be set to exclude reads where the alignment to the reference is mostly matching. This is only useful when aligning to a genome, since most transcriptomic reads should be missing intron sequences that are counted in the alignment as non-matches. This just blocks spurious alignments and short reads that don't appear to be attributable to a functional gene (potentially ncRNA, or potentially alignment errors). The setting for this is impirically set based on close observations of the data. This function outputs a csv file with each gene as a column and each row is a barcode and sample ID.
    Run map_count.add_counts() as needed to combine the counts csv files from multiple sequencing runs into one.
    At this stage, you have data in terms of gene counts per sample/barcode, which can be directly used for statistical analyis. Two analytical functions are provided here under analysis.py for rudimentary comparison and filtering of data between samples.
For analysis:
    analysis.compare_counts() can be used to compare the gene counts between two samples in a single csv file and perform Welch's t-tests and generate volcano plots. This outputs the comparison as a table with each gene as a row and columns detailing the fold change, pvalue, Benjamin-Hochberg FDR, and other stats between the two samples. The samples to compare are provided as a list of two sample IDs that match to the sample IDs in your counts file. This comparison can be restricted using priors, essentially a whitelist, and neg_priors, essentially a blacklist. These must be given as dicts with gene_name:log_2_fold_change mappings. See the next entry for more details. The comparison can be limited to certain genes by providing a list of gene IDs to gene_names. Genes can also be filtered by the average count among the samples with min_avg_count, which helps remove genes with such low counts that they can't properly be quantified. sort_table_by can be used to sort the results table, and must be given as a list of columns and directions ie [('column_name', 'ascending')], and the sorting will be done starting with the first item in the list. 
    Priors can be created using analysis.generate_priors(), which uses compare_counts() to generate a list of genes and their log_2 fold changes to use as positive or negative priors in compare_counts. This can be very useful when comparing several samples that are expected to have correlated results. For example, if the significant gene regulations of Sample A vs Sample B are expected to be a subset of the results of Sample C vs Sample D, priors can be generated from CvsD and used to limit the comparison of AvsB, which can clear up the results and statistics. The inverse can also be helpful, where you want to exclude the significant results of a comparison from another. This function is very customizable, as you can specify the total number of genes to report with N, the sorting method with field_to_sort_by, and functions provided to funcs_to_check that work with corresponding fields in fields_to_check. These functions must be paired with fields (column names) specified in fields_to_check, and they will be evaluated together in order. The functions simply must take the value that would be found under their corresponding column for a single gene and return True or False. For example, providing funcs_to_check = [ lambda x : x > 1], fields_to_check = ['log_2_fold_change'] would limit the results to genes with a log_2_fold_change > 1. Any number of functions and fields can be specified. The any_or_all value determines whether genes must meet at least one of the provided functions or must meet all of them in order to be included. The dict returned by this function can directly be given to compare_counts() as priors or neg_priors. When given as priors, the resulting comparison and FDR correction will only be done on the genes in priors, and the resulting plot will show colors corresponding to the sign of log_2_fold_change for the genes in priors (ie genes upregulated in priors will be red, downregulated will be blue). When given as neg_priors, the genes listed will be shown in black on plots and will have their padj set to 1, but only if the sign of log_2_fold_change matches between the neg_priors and the current comparison. neg_priors doesn't affect the padj calculation of any other genes.
    analysis.qc_metrics() can be used to get some quick stats about the barcode multiplexing. This will show separate histograms of read lengths for reads that had itentifiable barcodes and 5' SSPs, an identifiable barcode but no 5' SSP, and reads that had no identifiable barcode. This will also show a bar plot of each barcode's total number of reads with and without the 5' SSP.